---
title: "Deepfake detection challenge from R"
description: |
  A couple of months ago AWS, Facebook, Microsoft, and other contributors brought a challenge where users have to build a model to differentiate very realistic AI-generated fake videos. Despite the complexity and uniqueness of the computer vision task, we will show how the R community can also work with video dataset and bring their own solution.
author:
  - name: Turgut Abdullayev 
    url: https://github.com/henry090
    affiliation: QSS Analytics
    affiliation_url: http://www.qss.az/
date: 08-11-2020
categories:
  - Image Recognition & Image
creative_commons: CC BY
repository_url: https://github.com/henry090/Deepfake-from-R
output: 
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 2
preview: files/frame_2.png
---



<style type="text/css">
.colab-root {
    display: inline-block;
    background: rgba(255, 255, 255, 0.75);
    padding: 4px 8px;
    border-radius: 4px;
    font-size: 11px!important;
    text-decoration: none;
    color: #aaa;
    border: none;
    font-weight: 300;
    border: solid 1px rgba(0, 0, 0, 0.08);
    border-bottom-color: rgba(0, 0, 0, 0.15);
    text-transform: uppercase;
    line-height: 16px;
}
span.colab-span {
    background-image: url(https://distill.pub/2020/growing-ca/images/colab.svg);
    background-repeat: no-repeat;
    background-size: 20px;
    background-position-y: 2px;
    display: inline-block;
    padding-left: 24px;
    border-radius: 4px;
    text-decoration: none;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Introduction

Working with video datasets, particularly with detection of AI-based fake objects is very challenging due to proper frame selection and face detection. However, the solution for this case can be found from the R side as well. For this purpose, one can benefit the opportunities of OpenCV, magick, and Keras libraries from R.

Before going into detailed explanation and code exploration, readers should know that there is no need to copy-paste code chunks. Because at the end of the post one can find a link to Google Colab with GPU acceleration. This Kernel allows everyone to run and reproduce the same results.


## Data exploration

[The dataset](https://www.kaggle.com/c/deepfake-detection-challenge/overview/description) that we are going to analyze is provided by AWS, Facebook, Microsoft, the Partnership on AIâ€™s Media Integrity Steering Committee, and academics.

It contains both real and AI-generated fake videos. The total size is over 470 GB. However, the sample 4 GB dataset is separately available.

## Frame extraction

The videos in the folders are in the format of _mp4_ and have various lengths. The caption of the right frames and definition of total frames for each second is out of the scope of this post. We usually took 1-3 fps for every video.

> Note: Set fps to NULL if you want to extract all frames.

```{r eval=F, echo=T}
video = magick::image_read_video("aagfhgtpmv.mp4",fps = 2)
vid_1 = video[[1]]
vid_1 = magick::image_read(vid_1) %>% image_resize('1000x1000')
```

<center>

```{r, eval=TRUE, echo=FALSE, layout="l-body", fig.cap = "[Deepfake detection challenge](https://www.kaggle.com/c/deepfake-detection-challenge/data)"}
knitr::include_graphics("files/frame_1.png")
```

</center>

We saw just the first one. What about the rest of them?

```{r, eval=TRUE, echo=FALSE, layout="l-body", fig.cap = "[Deepfake detection challenge](https://www.kaggle.com/c/deepfake-detection-challenge/data)"}
knitr::include_graphics("files/self.gif")
```

Looking at the gif one can observe that some fakes are very easy to differentiate but a small fraction is too similar to real ones but has a less fake effect on the face of the person. This is another challenge during data preparation.


## Face detection

We can read all the files or take a sample. And afterward, detect and extract faces. 

There is a very well-known library Opencv^[[Opencv](https://opencv.org/)] which is designed for computer vision tasks. R has also some bindings to Opencv, so extracting faces will not be a challenge for us.

At first, face locations need to be determined via bounding boxes with Opencv and then with magick library automatically extracted from all of the images.

```{r eval=F, echo=T}
# get face location and calculate bounding box
library(opencv)
unconf <- ocv_read('frame_1.png')
faces <- ocv_face(unconf)
facemask <- ocv_facemask(unconf)
df = attr(facemask, 'faces')
rectX = (df$x - df$radius) 
rectY = (df$y - df$radius)
x = (df$x + df$radius) 
y = (df$y + df$radius)

# draw with red dashed line the box
imh  = image_draw(image_read('frame_1.png'))
rect(rectX, rectY, x, y, border = "red", 
     lty = "dashed", lwd = 2)
dev.off()

```

<center>

```{r, eval=TRUE, echo=FALSE, layout="l-body", fig.cap = "[Deepfake detection challenge](https://www.kaggle.com/c/deepfake-detection-challenge/data)"}
knitr::include_graphics("files/frame_2.png")
```

</center>

## Face extraction

If face locations are found, then it is very easy to extract them all. 

```{r eval=F, echo=T}
edited = image_crop(imh, "49x49+66+34")
edited = image_crop(imh, paste(x-rectX+1,'x',x-rectX+1,'+',rectX, '+',rectY,sep = ''))
edited
```

<center>

```{r, eval=TRUE, echo=FALSE, layout="l-body", fig.cap = "[Deepfake detection challenge](https://www.kaggle.com/c/deepfake-detection-challenge/data)"}
knitr::include_graphics("files/frame_1_face.png")
```

</center>
 
 
## Bringing a solution

After dataset preparation, it is time to build a deep learning model with Keras. We can quickly place all the images to folders and using image generators feed faces to a pre-trained Keras model. 

```{r eval=F,echo=T}
train_dir = 'fakes_reals'
width = 150L
height = 150L
epochs = 10

train_datagen = image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest",
  validation_split=0.2
)


train_generator <- flow_images_from_directory(
  train_dir,                  
  train_datagen,             
  target_size = c(width,height), 
  batch_size = 10,
  class_mode = "binary"
)

# Build the model ---------------------------------------------------------

conv_base <- application_vgg16(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(width, height, 3)
)

model <- keras_model_sequential() %>% 
  conv_base %>% 
  layer_flatten() %>% 
  layer_dense(units = 256, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
)

history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = ceiling(train_generator$samples/train_generator$batch_size),
  epochs = 10
)
```
 
<center>

<br><br> <a href="https://colab.research.google.com/drive/1Wf9aTdcC_YtigjQIYcG8zJqq2q_vpZR9?usp=sharing" class="colab-root">Reproduce in a <span class="colab-span">Notebook</span></a>

</center>

## Conclusion

This post shows how the R community can work with video data and bring a solution to the computer vision tasks. However, readers should know that the implementation of the following steps may drastically improve model performance:

- extract of all of the frames from video files
- upload different pre-trained weights from Keras library
- use another technology to detect faces -- "MTCNN face detector"
- use image ops from TensorFlow Addons^[[TensorFlow Addons](https://github.com/henry090/tfaddons)] for better fake face detection
- and in many other ways.

Try these options on the Deepfake detection challenge and share your results in the comment section!



